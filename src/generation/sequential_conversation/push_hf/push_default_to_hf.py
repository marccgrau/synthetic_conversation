#!/usr/bin/env python3
import glob
import json
import os

from huggingface_hub import create_repo, upload_file


def combine_json_files(input_directory: str, output_file: str) -> None:
    """
    Reads all JSON files from input_directory, combines the conversation records,
    and writes them to output_file in JSON Lines (JSONL) format.
    """
    combined_conversations = []
    json_files = glob.glob(os.path.join(input_directory, "*.json"))
    print(f"Found {len(json_files)} JSON files in {input_directory}.")

    for filepath in json_files:
        with open(filepath, "r", encoding="utf-8") as f:
            try:
                data = json.load(f)
                if isinstance(data, list):
                    combined_conversations.extend(data)
                else:
                    print(f"Skipping {filepath}: Not a list of records.")
            except Exception as e:
                print(f"Error reading {filepath}: {e}")

    # Write each conversation record as a single JSON line
    with open(output_file, "w", encoding="utf-8") as f:
        for record in combined_conversations:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")
    print(f"Combined {len(combined_conversations)} records into {output_file}.")


def push_to_huggingface(repo_name: str, file_path: str, token: str) -> None:
    """
    Creates a dataset repository (if it doesn't exist) and uploads the file to it.
    """
    try:
        # Create the repository; exist_ok=True avoids errors if it already exists.
        create_repo(repo_name, repo_type="dataset", exist_ok=True, token=token)
        print(f"Repository '{repo_name}' is ready.")
    except Exception as e:
        print(f"Error creating repo: {e}")

    # Upload the file using the modern upload_file function
    try:
        upload_file(
            path_or_fileobj=file_path,
            path_in_repo=os.path.basename(file_path),
            repo_id=repo_name,
            repo_type="dataset",
            token=token,
        )
        print(f"Uploaded '{file_path}' to repository '{repo_name}'.")
    except Exception as e:
        print(f"Error uploading {file_path}: {e}")


def main():
    # Define directories and file names
    input_directory = "agentic_simulation_outputs"
    output_file = "conversations_dataset.jsonl"

    # Combine all JSON files into a single JSONL file
    combine_json_files(input_directory, output_file)

    # Retrieve your Hugging Face token from environment variables
    hf_token = os.environ.get("HF_TOKEN")
    if not hf_token:
        raise ValueError(
            "Please set the HF_TOKEN environment variable with your Hugging Face API token."
        )

    # Define your repository name (update "your-username" accordingly)
    repo_name = "marccgrau/agentic_synthetic_customer_service_conversations"

    # Create a dataset card as a README file including metadata and guidance.
    readme_content = """---
license: "apache-2.0"
pretty_name: "Agentic Simulated Customer Service Conversations"
language: "de"
task_categories:
  - text-generation
  - summarization
size_categories: ["unknown"]
tags:
  - simulation
  - synthetic
  - customer_service
multilinguality: monolingual
language_creators:
  - synthetic
---

# Agentic Simulated Customer Service Conversations Dataset

## Overview
This dataset contains simulated conversations generated by our agentic simulation system.
Each record is stored in JSON Lines (JSONL) format and includes:
- **Input Settings:** Metadata such as selected bank, customer, agent profiles, and task details.
- **Messages:** The full conversation messages.
- **Summary:** A German summary of the conversation.
- **Cost Information:** API cost metrics for the conversation simulation.

## Intended Use
The dataset is designed for analysis of customer service interactions and as training data for conversational agents.
Users should note that the dataset is synthetically generated and may exhibit biases based on the simulation parameters.

## Potential Biases and Limitations
While efforts were made to simulate realistic interactions, the conversations are generated based on predefined scenarios and random sampling.
Users should exercise caution when using this dataset for real-world applications and be aware of potential biases introduced during simulation.

## Additional Information
For more details on the dataset creation process or any questions, please contact the dataset maintainers.
"""
    readme_file = "README.md"
    with open(readme_file, "w", encoding="utf-8") as f:
        f.write(readme_content)
    print(f"Created dataset card and README file '{readme_file}' with metadata.")

    # Push the combined dataset file and README to Hugging Face
    push_to_huggingface(repo_name, output_file, hf_token)
    push_to_huggingface(repo_name, readme_file, hf_token)


if __name__ == "__main__":
    main()
